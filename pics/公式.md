# Attention Is All You Need

**Attention**
$$
\text{Attention}(Q,K,V) = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}})V
$$
其中$Q,K \in R^{\cdot × d_k}$, $V \in R^{\cdot × d_v}$. 对于不同的attn层, $Q,K,V$ 由来如下:

- 对于 encoder 的 attn, $Q=XW^q, K=XW^k, V=XW^v$
- 对于 decoder 的 masked attn, 三者均来自于上个 $t$ 时刻的输出 $h_{decoder}^t$ 分别做类似第1步的操作所得
- 对于 decoder 的 Encoder-Decoder Attn, $K,V$来自于 encoder 的最后输出的 $h_{encoder}^L$ 做第1步的操作所得, 而 $Q$ 来于 decoder 的前一层的输出

**Multi-head Attn**
$$
\begin{split}
\text{MultiHead}(Q,K,V) &= \text{Concat{head$_1,\dots,$head$_h$}} W^O \\
其中\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{split}
$$
其中 $W_i^Q \in R^{d_{model} × d_k}, W_i^K \in R^{d_{model} × d_k}, W_i^V \in R^{d_{model} × d_v}, W_i^O \in R^{hd_v× d_{model}}$

**Position-wise Feed-Forward Networks**
$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$
**Each sub-layer**
$$
y = \text{LayerNorm}(x + \text{Sublayer}(x))
$$
**Positional Encoding**
$$
\begin{split}
\text{PE}_{(pos, 2i)} = \sin(pos / 10000^{2^i / d_{model}}) \\
\text{PE}_{(pos, 2i+1)} = \cos(pos / 10000^{2^i / d_{model}}) 
\end{split}
$$


# Improving Language Understanding by Generative Pre-Training

给定unlabeled corpus $\mathcal{U}=\{u_1,\dots,u_2\}$, 优化如下**standard language modeling objective**:
$$
L_1(\mathcal{U}) = \max_{\Theta}\sum_i\log P(u_i|u_{i-k},\dots,u_{i-1};\Theta)
$$
其中$k$是 context window的大小, $P(\cdot)$如下:
$$
\begin{split}
h_0 &= U W_e + W_p \\
h_i &= \text{Transformer_block}(h_{i-1}), \ \ \forall i \in [1,n] \\
P(u) &= \text{softmax}(h_n W_e^T)
\end{split}
$$
其中$U=\{u{-k},\dots,u_{-1}\} \in R^{k×d}$ 是 context vector of tokens (每个$u$即为一个one-hot向量, $d$表示字典的大小), $n$是multi-layer Transformer的层数(即每个Transformer_block为一层Transformer), $W_e \in R^{d × s}$是token embedding matrix, $W_p$是position embedding embedding matrix



给定 target task 里的 labeled dataset $\mathcal{C}=\{(x,y)\}$, 其中 $x=<x^1,\dots,x^m>$是 a sequence of input tokens, $y$ 是 label, 则优化如下目标:
$$
\begin{split}
L_2(\mathcal{C}) &= \max_{W_y}\sum_{(x,y)}\log P(y|x^1,\dots,x^m) \\
&= \max_{W_y}\sum_{(x,y)}\text{softmax}(h_n W_y)
\end{split}
$$
实验发现若该优化目标添加上 language modeling 的话, 有助于提高 supervised model 的泛化性, 同时能加速收敛. 因此最终的优化目标为:
$$
L3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda L_1(\mathcal{C})
$$
注意, 整个过程仅fine tune $W_y$ 以及 embeddings for delimiter tokens



