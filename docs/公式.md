# Attention Is All You Need

**Attention**
$$
\text{Attention}(Q,K,V) = \text{softmax}(\frac{Q K^T}{\sqrt{d_k}})V
$$
其中$Q,K \in R^{\cdot × d_k}$, $V \in R^{\cdot × d_v}$. 对于不同的attn层, $Q,K,V$ 由来如下:

- 对于 encoder 的 attn, $Q=XW^q, K=XW^k, V=XW^v$
- 对于 decoder 的 masked attn, 三者均来自于上个 $t$ 时刻的输出 $h_{decoder}^t$ 分别做类似第1步的操作所得
- 对于 decoder 的 Encoder-Decoder Attn, $K,V$来自于 encoder 的最后输出的 $h_{encoder}^L$ 做第1步的操作所得, 而 $Q$ 来于 decoder 的前一层的输出

**Multi-head Attn**
$$
\begin{split}
\text{MultiHead}(Q,K,V) &= \text{Concat{head$_1,\dots,$head$_h$}} W^O \\
其中\text{head}_i &= \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
\end{split}
$$
其中 $W_i^Q \in R^{d_{model} × d_k}, W_i^K \in R^{d_{model} × d_k}, W_i^V \in R^{d_{model} × d_v}, W_i^O \in R^{hd_v× d_{model}}$

**Position-wise Feed-Forward Networks**
$$
\text{FFN}(x) = \max(0, xW_1 + b_1) W_2 + b_2
$$
**Each sub-layer**
$$
y = \text{LayerNorm}(x + \text{Sublayer}(x))
$$
**Positional Encoding**
$$
\begin{split}
\text{PE}_{(pos, 2i)} = \sin(pos / 10000^{2^i / d_{model}}) \\
\text{PE}_{(pos, 2i+1)} = \cos(pos / 10000^{2^i / d_{model}}) 
\end{split}
$$


# Improving Language Understanding by Generative Pre-Training

给定unlabeled corpus $\mathcal{U}=\{u_1,\dots,u_2\}$, 优化如下**standard language modeling objective**:
$$
L_1(\mathcal{U}) = \max_{\Theta}\sum_i\log P(u_i|u_{i-k},\dots,u_{i-1};\Theta)
$$
其中$k$是 context window的大小, $P(\cdot)$如下:
$$
\begin{split}
h_0 &= U W_e + W_p \\
h_i &= \text{Transformer_block}(h_{i-1}), \ \ \forall i \in [1,n] \\
P(u) &= \text{softmax}(h_n W_e^T)
\end{split}
$$
其中$U=\{u{-k},\dots,u_{-1}\} \in R^{k×d}$ 是 context vector of tokens (每个$u$即为一个one-hot向量, $d$表示字典的大小), $n$是multi-layer Transformer的层数(即每个Transformer_block为一层Transformer), $W_e \in R^{d × s}$是token embedding matrix, $W_p$是position embedding embedding matrix



给定 target task 里的 labeled dataset $\mathcal{C}=\{(x,y)\}$, 其中 $x=<x^1,\dots,x^m>$是 a sequence of input tokens, $y$ 是 label, 则优化如下目标:
$$
\begin{split}
L_2(\mathcal{C}) &= \max_{W_y}\sum_{(x,y)}\log P(y|x^1,\dots,x^m) \\
&= \max_{W_y}\sum_{(x,y)}\text{softmax}(h_n W_y)
\end{split}
$$
实验发现若该优化目标添加上 language modeling 的话, 有助于提高 supervised model 的泛化性, 同时能加速收敛. 因此最终的优化目标为:
$$
L3(\mathcal{C}) = L_2(\mathcal{C}) + \lambda L_1(\mathcal{C})
$$
注意, 整个过程仅fine tune $W_y$ 以及 embeddings for delimiter tokens



# 统计学习

$$
L(L,f(x)) = \begin{cases}
1, & f(x) \neq Y \\
0, & f(x) = Y
\end{cases} \\
L(L,f(x)) =(Y - f(X)^2 \\
L(L,f(x)) = |Y - f(X)| \\
L(L,f(x)) = -\log P(Y|X) \\
R_{exp}(f) = E_P[L(Y, f(X))] = \int_{\mathcal{X×Y}} L(y, f(x)) P(x, y) dx dy \approx \frac{1}{N} \sum_{i=1}^N L(y_i, f(x_i))
$$

$$
f(x) = \text{sign}(wx+b)
$$

其中 $w \in R^n$ 为权值/权值向量(weight), $b \in R$ 为偏置(bias), sign为符号函数

点 $x_0$ 到超平面 $wx+b=0$ 的距离公式为 $\frac{1}{\sqrt{\|w\|_2^2+b^2}}|wx_0+b|$. 为了简便计算, 这里通常忽略 $b$, 因此简化为 $\frac{1}{\|w\|_2} |wx_0+b|$. 

给定误分类的点集$\mathcal{X}_{neg}$, 对于$\forall x_i \in \mathcal{X}_{neg}$, 它到超平面的距离 $-\frac{1}{\|w\|_2} y_i (wx_i+b) > 0$. 因此$\mathcal{X}_{neg}$ 到超平面的距离为 $-\frac{1}{\|w\|_2} \sum_{(x_i, y_i) \in \mathcal{X}_{neg}}y_i (wx_i+b) > 0$. 不考虑 $\|w\|_2$, 则感知机的损失函数定义为:
$$
L(w,b) = \min_{w, b} -\sum_{(x_i, y_i) \in \mathcal{X}_{neg}} y_i (wx_i+b) > 0
$$
